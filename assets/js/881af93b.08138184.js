(window.webpackJsonp=window.webpackJsonp||[]).push([[23],{117:function(e,t,n){"use strict";n.d(t,"a",(function(){return u})),n.d(t,"b",(function(){return b}));var a=n(0),o=n.n(a);function r(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function i(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function l(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?i(Object(n),!0).forEach((function(t){r(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):i(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,o=function(e,t){if(null==e)return{};var n,a,o={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(o[n]=e[n]);return o}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(o[n]=e[n])}return o}var c=o.a.createContext({}),p=function(e){var t=o.a.useContext(c),n=t;return e&&(n="function"==typeof e?e(t):l(l({},t),e)),n},u=function(e){var t=p(e.components);return o.a.createElement(c.Provider,{value:t},e.children)},d={inlineCode:"code",wrapper:function(e){var t=e.children;return o.a.createElement(o.a.Fragment,{},t)}},m=o.a.forwardRef((function(e,t){var n=e.components,a=e.mdxType,r=e.originalType,i=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),u=p(n),m=a,b=u["".concat(i,".").concat(m)]||u[m]||d[m]||r;return n?o.a.createElement(b,l(l({ref:t},c),{},{components:n})):o.a.createElement(b,l({ref:t},c))}));function b(e,t){var n=arguments,a=t&&t.mdxType;if("string"==typeof e||a){var r=n.length,i=new Array(r);i[0]=m;var l={};for(var s in t)hasOwnProperty.call(t,s)&&(l[s]=t[s]);l.originalType=e,l.mdxType="string"==typeof e?e:a,i[1]=l;for(var c=2;c<r;c++)i[c]=n[c];return o.a.createElement.apply(null,i)}return o.a.createElement.apply(null,n)}m.displayName="MDXCreateElement"},93:function(e,t,n){"use strict";n.r(t),n.d(t,"frontMatter",(function(){return i})),n.d(t,"metadata",(function(){return l})),n.d(t,"toc",(function(){return s})),n.d(t,"default",(function(){return p}));var a=n(3),o=n(7),r=(n(0),n(117)),i={sidebar_position:4},l={unversionedId:"tutorial-extras/next-steps",id:"tutorial-extras/next-steps",isDocsHomePage:!1,title:"[ADVANCED] Next steps .... training a Deep Learning model for image segmentation",description:"(page under construction)",source:"@site/docs/tutorial-extras/next-steps.md",sourceDirName:"tutorial-extras",slug:"/tutorial-extras/next-steps",permalink:"/dash_doodler/docs/tutorial-extras/next-steps",editUrl:"https://github.com/dbuscombe-usgs/dash_doodler/edit/master/website/docs/tutorial-extras/next-steps.md",version:"current",sidebarPosition:4,frontMatter:{sidebar_position:4},sidebar:"tutorialSidebar",previous:{title:"[ADVANCED] How to contribute",permalink:"/dash_doodler/docs/tutorial-extras/how-to-contribute"},next:{title:"[ADVANCED] References",permalink:"/dash_doodler/docs/tutorial-extras/references"}},s=[{value:"Utility scripts",id:"utility-scripts",children:[]},{value:"Deep Learning for Image Segmentation",id:"deep-learning-for-image-segmentation",children:[]},{value:"U-Net",id:"u-net",children:[]}],c={toc:s};function p(e){var t=e.components,n=Object(o.a)(e,["components"]);return Object(r.b)("wrapper",Object(a.a)({},c,n,{components:t,mdxType:"MDXLayout"}),Object(r.b)("p",null,"(page under construction)"),Object(r.b)("h3",{id:"utility-scripts"},"Utility scripts"),Object(r.b)("p",null,"Doodler is compatible with my other segmentation program, ",Object(r.b)("a",{parentName:"p",href:"https://github.com/dbuscombe-usgs/segmentation_zoo"},"Zoo")," in a couple of different ways:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"You could run the function ",Object(r.b)("inlineCode",{parentName:"p"},"gen_npz_4_zoo.py")," to create npz files that contain only image and label pairs. This is the same output as you would get from running the Zoo program `make_datasets.py'")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"You could alternatively run the function ",Object(r.b)("inlineCode",{parentName:"p"},"gen_images_and_labels_4_zoo.py")," that would generate jpeg greyscale image files and image jpegs for use with the Zoo program `make_datasets.py'."))),Object(r.b)("p",null,"The first scenario might be most common because it requires one less step, however the second scenario might be useful for using the labels with another software package, or for further post-processing of the labels"),Object(r.b)("p",null,"There are two additional scripts in the ",Object(r.b)("inlineCode",{parentName:"p"},"utils")," folder:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},Object(r.b)("inlineCode",{parentName:"p"},"viz_npz.py")," creates transparent overlay plots of images and labels, and has three modes with the following syntax ",Object(r.b)("inlineCode",{parentName:"p"},"viz_npz.py [-t npz type {0}/1/2]")," where optional ",Object(r.b)("inlineCode",{parentName:"p"},"-t")," controls what type of npz file: native from doodler (option 0, default), a ",Object(r.b)("inlineCode",{parentName:"p"},"labelgen")," file from ",Object(r.b)("inlineCode",{parentName:"p"},"plot_label_generation.npz"),", a npz file used as input for Zoo")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},Object(r.b)("inlineCode",{parentName:"p"},"plot_label_generation.py")," that generates a detailed sequence of plots for every input npz file from doodler, including plots of the doodles themselves, overlays, and internal model outputs."))),Object(r.b)("h3",{id:"deep-learning-for-image-segmentation"},"Deep Learning for Image Segmentation"),Object(r.b)("p",null,"There are two main drawbacks with neural networks consisting of only dense layers for classifying images:"),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"Too many parameters means even simple networks on small and low-dimensional imagery take a long time to train")),Object(r.b)("li",{parentName:"ul"},Object(r.b)("p",{parentName:"li"},"There is no concept of spatial distance, so the relative proximity of image features or that with respect to the classes, is not considered"))),Object(r.b)("p",null,'For both the reasons above, Convolutional Neural Networks or CNNs have become popular for working with imagery. CNNs train in a similar way to "fully connected" ANNs that use Dense layers throughout.'),Object(r.b)("p",null,"CNNs can handle multidimensional data as inputs, meaning we can use 3- or more band imagery, which is typical in the geosciences. Also, each neuron isn't connected to all others, only those within a region called the receptive field of the neuron, dictated by the size of the convolution filter used within the layer to extract features from the image. By linking only subsets of neurons, there are far fewer parameters for a given layer"),Object(r.b)("p",null,"So, CNNS take advantage of both multidimensionality and local spatial connectivity"),Object(r.b)("p",null,"CNNs tend to take relatively large imagery and extract smaller and smaller feature maps. This is achieved using pooling layers, which find and compress the features in the feature maps outputted by the CNN layers, so images get smaller and features are more pronounced"),Object(r.b)("h2",{id:"u-net"},"U-Net"),Object(r.b)("p",null,"Introduced in 2015, the U-Net model is relatively old in the deep learning field (!) but still popular and is also commonly seen embedded in more complex deep learning models"),Object(r.b)("p",null,"The U-Net model is a simple fully convolutional neural network that is used for binary segmentation i.e foreground and background pixel-wise classification. Mainly, it consists of two parts."),Object(r.b)("ul",null,Object(r.b)("li",{parentName:"ul"},"Encoder: we apply a series of conv layers and downsampling layers (max-pooling) layers to reduce the spatial size"),Object(r.b)("li",{parentName:"ul"},"Decoder: we apply a series of upsampling layers to reconstruct the spatial size of the input.\nThe two parts are connected using a concatenation layers among different levels. This allows learning different features at different levels. At the end we have a simple conv 1x1 layer to reduce the number of channels to 1.")),Object(r.b)("p",null,'U-Net is symmetrical (hence the "U" in the name) and uses concatenation instead of addition to merge feature maps'),Object(r.b)("p",null,"The encoder (left hand side of the U) downsamples the N x N x 3 image progressively using six banks of convolutional filters, each using filters double in size to the previous, thereby progressively downsampling the inputs as features are extracted through max pooling"),Object(r.b)("p",null,"A 'bottleneck' is just machine learning jargon for a very low-dimensional feature representation of a high dimensional input. Or, a relatively small vector of numbers that distill the essential information about a large image An input of N x N x 3 (>>100,000 numbers) has been distilled to a 'bottleneck' of 16 x 16 x M (<<100,000 numbers)"),Object(r.b)("p",null,"The decoder (the right hand side of the U) upsamples the bottleneck into a N x N x 1 label image progressively using six banks of convolutional filters, each using filters half in size to the previous, thereby progressively upsampling the inputs as features are extracted through transpose convolutions and concatenation. A transposed convolution is a relatively new type of deep learning model layer that convolves a dilated version of the input tensor, in order to upscale the output. The dilation operation consists of interleaving zeroed rows and columns between each pair of adjacent rows and columns in the input tensor. The dilation rate is the stride length"),Object(r.b)("p",null,"Finally, make the classification layer using one final convolutional layers that essentially just maps (by squishing over 16 layers) the output of the previous layer to a single 2D output (with values ranging from 0 to 1) based on a sigmoid activation function"))}p.isMDXComponent=!0}}]);