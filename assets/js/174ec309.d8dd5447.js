(window.webpackJsonp=window.webpackJsonp||[]).push([[8],{126:function(e,t,a){"use strict";a.d(t,"a",(function(){return u})),a.d(t,"b",(function(){return m}));var i=a(0),r=a.n(i);function s(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function n(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);t&&(i=i.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,i)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?n(Object(a),!0).forEach((function(t){s(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):n(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function l(e,t){if(null==e)return{};var a,i,r=function(e,t){if(null==e)return{};var a,i,r={},s=Object.keys(e);for(i=0;i<s.length;i++)a=s[i],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var s=Object.getOwnPropertySymbols(e);for(i=0;i<s.length;i++)a=s[i],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var d=r.a.createContext({}),c=function(e){var t=r.a.useContext(d),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},u=function(e){var t=c(e.components);return r.a.createElement(d.Provider,{value:t},e.children)},p={inlineCode:"code",wrapper:function(e){var t=e.children;return r.a.createElement(r.a.Fragment,{},t)}},b=r.a.forwardRef((function(e,t){var a=e.components,i=e.mdxType,s=e.originalType,n=e.parentName,d=l(e,["components","mdxType","originalType","parentName"]),u=c(a),b=i,m=u["".concat(n,".").concat(b)]||u[b]||p[b]||s;return a?r.a.createElement(m,o(o({ref:t},d),{},{components:a})):r.a.createElement(m,o({ref:t},d))}));function m(e,t){var a=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var s=a.length,n=new Array(s);n[0]=b;var o={};for(var l in t)hasOwnProperty.call(t,l)&&(o[l]=t[l]);o.originalType=e,o.mdxType="string"==typeof e?e:i,n[1]=o;for(var d=2;d<s;d++)n[d]=a[d];return r.a.createElement.apply(null,n)}return r.a.createElement.apply(null,a)}b.displayName="MDXCreateElement"},179:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/paperfig_RFchain_ann-b56a8371bc87b77b219a9e0ca2226667.jpg"},180:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_image_doodles_labelgen-e548fceb28a327b50a42c49e1d3af736.png"},181:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_image_filt_labelgen-505bc32ee695274f105461ba311a8235.png"},182:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_image_feats_labelgen-1fdb12ad2e7666a13412a34463fa5fc9.png"},183:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_RFdecsurf_labelgen_ann-c6ae8f7b89f946437b6f998740e9a2b7.png"},184:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_RF_featimps_labelgen-ffd0c404f415c9d4cc8c44eb96d5f5ff.png"},185:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/featimps1-9741b2a1182b6a7e4124969f93bb5c6c.png"},186:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/featimps2-db1d2704c520af94fb52f3a669a174e8.png"},187:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/featimps3-b6adcc6f8879cbc7b75a4be288e90e88.png"},188:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_rf_label_filtered_labelgen-4a390b5e15fdb1eda7e8e7070960f6bc.png"},189:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_rf_spatfilt_dist_labelgen_ann-d343c8e126bbdc7114ddbf63d199f0c0.png"},190:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_crf_tta_labelgen-4cb80119f53fcfac89105351cbfadbdd.png"},191:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_crf_label_filtered_labelgen-ee93ef47d939db64179db2323637f279.png"},192:function(e,t,a){"use strict";a.r(t),t.default=a.p+"assets/images/D800_20160308_222129lr03-3_db_image_label_final_labelgen-9a4b346d54d7a617c37c1e993fd94dff.png"},74:function(e,t,a){"use strict";a.r(t),a.d(t,"frontMatter",(function(){return n})),a.d(t,"metadata",(function(){return o})),a.d(t,"toc",(function(){return l})),a.d(t,"default",(function(){return c}));var i=a(3),r=a(7),s=(a(0),a(126)),n={sidebar_position:1},o={unversionedId:"tutorial-extras/how-doodler-works",id:"tutorial-extras/how-doodler-works",isDocsHomePage:!1,title:"[ADVANCED] How Doodler works",description:"(Please note that this material will be included in a forthcoming journal manuscript that describes Doodler and its uses. That manuscript is currently in preparation and will repurpose many of the figures presented on this page)",source:"@site/docs/tutorial-extras/how-doodler-works.md",sourceDirName:"tutorial-extras",slug:"/tutorial-extras/how-doodler-works",permalink:"/dash_doodler/docs/tutorial-extras/how-doodler-works",editUrl:"https://github.com/dbuscombe-usgs/dash_doodler/edit/master/website/docs/tutorial-extras/how-doodler-works.md",version:"current",sidebarPosition:1,frontMatter:{sidebar_position:1},sidebar:"tutorialSidebar",previous:{title:"Terminology",permalink:"/dash_doodler/docs/tutorial-basics/glossary"},next:{title:"[ADVANCED] Serving Doodler as a web application for others to use",permalink:"/dash_doodler/docs/tutorial-extras/deploy-server"}},l=[{value:"Overview",id:"overview",children:[]},{value:"Sparse annotation or &#39;doodling&#39;",id:"sparse-annotation-or-doodling",children:[]},{value:"Image standardization",id:"image-standardization",children:[]},{value:"Feature extraction",id:"feature-extraction",children:[]},{value:"Random Forest Modeling",id:"random-forest-modeling",children:[]},{value:"Feature importances",id:"feature-importances",children:[]},{value:"Spatial filtering of Random Forest predictions",id:"spatial-filtering-of-random-forest-predictions",children:[]},{value:"Conditional Random Field Modeling",id:"conditional-random-field-modeling",children:[]},{value:"Spatial filtering of CRF prediction",id:"spatial-filtering-of-crf-prediction",children:[]}],d={toc:l};function c(e){var t=e.components,n=Object(r.a)(e,["components"]);return Object(s.b)("wrapper",Object(i.a)({},d,n,{components:t,mdxType:"MDXLayout"}),Object(s.b)("p",null,"(Please note that this material will be included in a forthcoming journal manuscript that describes Doodler and its uses. That manuscript is currently in preparation and will repurpose many of the figures presented on this page)"),Object(s.b)("p",null,"Images are labeled in sessions. During a session, a Machine Learning model is built progressively using provided labels from each image. Below, we illustrate each of the various processing steps in turn, using a single example of a 2-class labeling exercise. The two classes are 'land' (red) and 'water' (blue)."),Object(s.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(s.b)("div",{parentName:"div",className:"admonition-heading"},Object(s.b)("h5",{parentName:"div"},Object(s.b)("span",{parentName:"h5",className:"admonition-icon"},Object(s.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(s.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tip")),Object(s.b)("div",{parentName:"div",className:"admonition-content"},Object(s.b)("p",{parentName:"div"},"Doodler 'learns as you go'. In a Doodler session, the skill of predictions usually improves the more images you doodle in a single session. That's because it updates the model with each set if new image feature-class pairings you provide it"))),Object(s.b)("h3",{id:"overview"},"Overview"),Object(s.b)("p",null,"The figure below depicts a typical Doodler session in which images (from left to right) are labeled sequentially. The figure is read from left to right, depicting the sequential nature of image labeling, as well as from top to bottom, which depicts the sequence of processes that occur to collectively turn the image at the top, to the label image at the bottom."),Object(s.b)("p",null,Object(s.b)("img",{src:a(179).default})),Object(s.b)("p",null,"There is a lot going on in this figure, and it is read from top to bottom (single image) and from left to right (a sequence of images in a session), so let's break it down a little ..."),Object(s.b)("h3",{id:"sparse-annotation-or-doodling"},"Sparse annotation or 'doodling'"),Object(s.b)("p",null,"It all begins with your inputs - doodles. This is what the subsequent list of operations are entirely based upon, together with some spatial logic and some assumptions regarding the types of 'features' to extract from imagery that would collectively best predict the classes."),Object(s.b)("p",null,Object(s.b)("img",{src:a(180).default})),Object(s.b)("h3",{id:"image-standardization"},"Image standardization"),Object(s.b)("p",null,"Doodler first standardizes the images, which means every pixel value is scaled to have a mean of zero and a variance of 1, or 'unit' variance. For the model, this ensures every image has a similar data distribution. It also acts like a 'white balance' filter for the image, enhancing color and contrast."),Object(s.b)("p",null,Object(s.b)("img",{src:a(181).default})),Object(s.b)("h3",{id:"feature-extraction"},"Feature extraction"),Object(s.b)("p",null,"Doodler estimates a dense (i.e. per-pixel) label image from an input image and your sparse annotations or 'doodles'. It does this by first extracting image features in a prescribed way (i.e. the image features are extracted in the same way each time) and matching those features to classes using Machine Learning."),Object(s.b)("p",null,"Doodler extracts a series of 2D feature maps from the standardized input imagery. From each sample image, 75 2D image feature maps are extracted (5 feature types, across 15 unique spatial scales)"),Object(s.b)("p",null,"The 5 extracted feature types are (from left to right in the image below):"),Object(s.b)("ul",null,Object(s.b)("li",{parentName:"ul"},"Relative location: the distance in pixels of each pixel to the image origin"),Object(s.b)("li",{parentName:"ul"},"Intensity: Gaussian blur over a range of scales"),Object(s.b)("li",{parentName:"ul"},"Edges: Sobel filter of the Gaussian blurred images"),Object(s.b)("li",{parentName:"ul"},"Primary texture: Matrix of texture values extacted over a range of scales as the 1st eigenvalue of the Hessian Matrix"),Object(s.b)("li",{parentName:"ul"},"Secondary texture: Matrix of texture values extacted over a range of scales as the 2nd eigenvalue of the Hessian Matrix")),Object(s.b)("p",null,"In the figure below, only the 5 feature maps extracted at the smallest and largest scales are shown for brevity:"),Object(s.b)("p",null,Object(s.b)("img",{src:a(182).default})),Object(s.b)("h3",{id:"random-forest-modeling"},"Random Forest Modeling"),Object(s.b)("p",null,"As stated above, Doodler extracts features from imagery, and pairs those extracted features with their class distinctions provided by you in the form of 'doodles'. How that pairing occurs is achieved using Machine Learning, or 'ML' for short. Doodler uses two particular types of ML. The first is called a 'Random Forest', or RF for short. The seond ML model we use is called a \"CRF' and we'll talk about that later."),Object(s.b)("p",null,'The first image is "doodled", and the program creates a ',Object(s.b)("a",{parentName:"p",href:"../tutorial-basics/glossary#random-forest"},"Random Forest")," that predicts the class of each pixel according to the distribution of features extracted from the vicinity of that pixel."),Object(s.b)("p",null,"Those 2D features are then flattened to a 1D array of length M, and stack them N deep, where N is the number of individual 2D feature maps, such that the resulting feature stack is size MxN. Provided label annotations are provided at a subset, i, of the M locations, M_i, so the training data is the subset {MxN}_i. That training data is further subsampled by a user-defined factor, then used to train a RF classifier."),Object(s.b)("p",null,"Below is a graphic showing, for this particular sample image we are using in this example, how the trained RF model is making decisions based on pairs of input features. Each of the 9 subplots shown depict a 'decision surface' for the two classes (water in blue and land in red) based on a pair of features. The colored markers show actual feature values extracted from image-feature-class pairings. As you can see, the RF model can extrapolate a decision surface beyond the extents of the data, which is useful in situations when data is encountered with relatively unusual feature values."),Object(s.b)("p",null,Object(s.b)("img",{src:a(183).default})),Object(s.b)("p",null,"In reality, the RF model does this on all 75 features and their respective combinations (2775 unique pairs of 75 features) simultaneously. It combines this information to predict a unique class (encoded as an integer value) for each pixel. The computations happen in 1D, i.e. on arrays of length MxN, which are then reshaped. Therefore the only spatial information used in prediction is that of the relative location feature maps."),Object(s.b)("p",null,"Some constraints are placed on the RF model in order to reduce the effects of overfitting the model to the data:"),Object(s.b)("ol",null,Object(s.b)("li",{parentName:"ol"},"The use of an ensemble model like as RF means that we rely on aggregating the results of an ensemble of simpler estimators (i.e. decision trees). An average of the outputs of an ensemble of parallel estimators, results in less overfitting."),Object(s.b)("li",{parentName:"ol"},"The features are downsampled by some user-defined factor. This also considerably speeds up processing."),Object(s.b)("li",{parentName:"ol"},"A cap is placed on the number of new trees used for each new image. That cap is user-defined and defaults to 3. See ",Object(s.b)("a",{parentName:"li",href:"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"},"this")," for more details."),Object(s.b)("li",{parentName:"ol"},"A minimum number of samples per decision tree split are used. That cap is currently set to 5. See ",Object(s.b)("a",{parentName:"li",href:"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"},"this")," for more details."),Object(s.b)("li",{parentName:"ol"},"A balanced subsample is used that is weighted inversely proportional to class frequencies in the input data. The weights are computed based on the bootstrap sample for every tree grown. See ",Object(s.b)("a",{parentName:"li",href:"https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"},"this")," for more details."),Object(s.b)("li",{parentName:"ol"},"A cap is placed on the number of features used by the model. That cap is currently set to 200,000")),Object(s.b)("p",null,"The use of multiple overfitting estimators can be combined to reduce the effect of this overfitting \u2014 this is what underlies an ensemble method called bagging.\nBagging makes use of an ensemble of parallel estimators, each of which over-fits the data, and averages the results to find a better classification."),Object(s.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(s.b)("div",{parentName:"div",className:"admonition-heading"},Object(s.b)("h5",{parentName:"div"},Object(s.b)("span",{parentName:"h5",className:"admonition-icon"},Object(s.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(s.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tip")),Object(s.b)("div",{parentName:"div",className:"admonition-content"},Object(s.b)("p",{parentName:"div"},"The program adopts a strategy similar to that described by ",Object(s.b)("a",{parentName:"p",href:"https://www.mdpi.com/2076-3263/8/7/244"},"Buscombe and Ritchie (2018)"),", in that a 'global' model trained on many samples is used to provide an initial segmentation on each sample image, then that initial segmentation is refined by a CRF, which operates on a task specific level. In ",Object(s.b)("a",{parentName:"p",href:"references"},"Buscombe and Ritchie (2018)"),", the model was a deep neural network trained in advance on large numbers of samples and labels. Here, the model is built as we go, building progressively from user inputs. Doodler uses a Random Forest as the baseline global model, and the CRF implementation is the same as that desribed by Buscombe and Ritchie (2018)"))),Object(s.b)("h3",{id:"feature-importances"},"Feature importances"),Object(s.b)("p",null,'The relative importance of each feature for prediction is computed as the mean of accumulation of the impurity decrease within each tree. This is known as the "Gini importance" score (see ',Object(s.b)("a",{parentName:"p",href:"https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"},"here")," for more details on the implementation)."),Object(s.b)("p",null,"Below is a graph showing the feature importance scores for each of the 75 features for this example image. The 4 highest scores are highlighted with a dashed red line. Those 4 feature maps are shown. In this particular example, the most important features for prediction relate to image intensity extracted over different scales."),Object(s.b)("p",null,Object(s.b)("img",{src:a(184).default})),Object(s.b)("p",null,"It is instructive to view these plots for each image prediction, in order to get a better sense of which features are considered more important. From 3 further examples shown below, it is apparent (at least in these data) that location, intensity, edges and texture are all considered important for RF model prediction, at a range of scales"),Object(s.b)("h4",{id:"example-2"},"Example 2"),Object(s.b)("p",null,"In this example, location and intensity were most important, at 2 particular scales\n",Object(s.b)("img",{src:a(185).default})),Object(s.b)("h4",{id:"example-3"},"Example 3"),Object(s.b)("p",null,"In this example, the features deemed most important were related to edges and texture\n",Object(s.b)("img",{src:a(186).default})),Object(s.b)("h4",{id:"example-4"},"Example 4"),Object(s.b)("p",null,"In this example, location and intensity were again deemed most important, again at 2 particular scales\n",Object(s.b)("img",{src:a(187).default})),Object(s.b)("h3",{id:"spatial-filtering-of-random-forest-predictions"},"Spatial filtering of Random Forest predictions"),Object(s.b)("p",null,"Each RF prediction (a label matrix of integer values, each integer corresponding to a unique class). That corresponds to the left image in the figure below. You can see there is a lot of noise in the prediction; for example, and most notably, there are several small 'islands' of land in the water that are model errors. Therefore each label image is filtered using two complementary procedures that operate in the spatial domain."),Object(s.b)("p",null,Object(s.b)("img",{src:a(188).default})),Object(s.b)("p",null,"The first filtering exercise (the outputs of which are labeled \"b) Filtered\" in the example figure above) operates on the one-hot encoded stack of labels. For each, pixel 'islands' less than a certain size are removed (filled in with the value of the surrounding area). Additionally, pixel 'holes' are also sealed (filled in with the value of the surrounding area). You can see in the example, by comparing a) and b) in the above figure, that many islands were removed in this process on this particular example."),Object(s.b)("p",null,"The second filter then determines a 'null class' based on those pixels that are furthest away from similar classes. Those pixels occur at the transition areas between large contiguous regions of same-class. The process by which this is acheived is described in the figure below:"),Object(s.b)("p",null,Object(s.b)("img",{src:a(189).default})),Object(s.b)("p",null,"The intuition for 'zeroing' these pixels is to allow a further model, described below, to estimate the appropriate class values for pixels in those transition areas."),Object(s.b)("h3",{id:"conditional-random-field-modeling"},"Conditional Random Field Modeling"),Object(s.b)("div",{className:"admonition admonition-tip alert alert--success"},Object(s.b)("div",{parentName:"div",className:"admonition-heading"},Object(s.b)("h5",{parentName:"div"},Object(s.b)("span",{parentName:"h5",className:"admonition-icon"},Object(s.b)("svg",{parentName:"span",xmlns:"http://www.w3.org/2000/svg",width:"12",height:"16",viewBox:"0 0 12 16"},Object(s.b)("path",{parentName:"svg",fillRule:"evenodd",d:"M6.5 0C3.48 0 1 2.19 1 5c0 .92.55 2.25 1 3 1.34 2.25 1.78 2.78 2 4v1h5v-1c.22-1.22.66-1.75 2-4 .45-.75 1-2.08 1-3 0-2.81-2.48-5-5.5-5zm3.64 7.48c-.25.44-.47.8-.67 1.11-.86 1.41-1.25 2.06-1.45 3.23-.02.05-.02.11-.02.17H5c0-.06 0-.13-.02-.17-.2-1.17-.59-1.83-1.45-3.23-.2-.31-.42-.67-.67-1.11C2.44 6.78 2 5.65 2 5c0-2.2 2.02-4 4.5-4 1.22 0 2.36.42 3.22 1.19C10.55 2.94 11 3.94 11 5c0 .66-.44 1.78-.86 2.48zM4 14h5c-.23 1.14-1.3 2-2.5 2s-2.27-.86-2.5-2z"}))),"Tip")),Object(s.b)("div",{parentName:"div",className:"admonition-content"},Object(s.b)("p",{parentName:"div"},"Already you can see how we are building a lot of robustness to natural variability:"),Object(s.b)("ol",{parentName:"div"},Object(s.b)("li",{parentName:"ol"},"images are standardized"),Object(s.b)("li",{parentName:"ol"},"RF models use balanced subsamples weighted by class frequency"),Object(s.b)("li",{parentName:"ol"},"Other measures are made to prevent model overfitting, such as downsampling"),Object(s.b)("li",{parentName:"ol"},"Different types of image and location features are used and extracted at a variety of scales"),Object(s.b)("li",{parentName:"ol"},"RF outputs are filtered using relative spatial information")),Object(s.b)("p",{parentName:"div"},"Next we'll go even further by making use of both global and local predictions."),Object(s.b)("p",{parentName:"div"},"The global predictions are provided by the RF model. They are called 'global' because the model is built from all doodled images in a sequence."),Object(s.b)("p",{parentName:"div"},"The local predictions are provided by a different type of ML model, called a CRF, which is explained below."))),Object(s.b)("p",null,"That initial RF provides an initial estimate of the entire label estimate, which is fed (with the original image) to a secondary post-processing model based on a fully connected Conditional Random Field model, or CRF for short. The CRF model refines the label image, using the RF model output as priors that are refined to posteriors given the specific image. As such, the RF model is treated as a global model that receives training input from the user over multiple images, and the CRF model is for 'local' (i.e. image-specific) refinement."),Object(s.b)("p",null,"The CRF builds a model for the likelihood of the RF-predicted labels based on the distributions of features it extracts from the imagery, and can reclassify pixels (it is intended to do so). Its feature extraction and decision making behavior is complex and governed by parameters. That prediction then is further refined by applying the model to numerous transformed versions of the image, making predictions, untransforming, and then averaging the stack of resulting predictions. This concept is called ",Object(s.b)("a",{parentName:"p",href:"../tutorial-basics/glossary#test-time-augmentation"},"test-time-augmentation")," and is illustrated in the figure below as outputs from using 10 'test-time augmented' inputs:"),Object(s.b)("p",null,Object(s.b)("img",{src:a(190).default})),Object(s.b)("h3",{id:"spatial-filtering-of-crf-prediction"},"Spatial filtering of CRF prediction"),Object(s.b)("p",null,"The label image that is the result of the above process is yet further filtered using the same two-part spatial procedure described above for the RF model outputs. Usually, these procedures revert fewer pixel class values than the equivalent prior process on the RF model outputs. The outputs are shown in 'b) and c)' in the figure below. A final additional step not used on RF model outputs is to 'inpaint' the pixels in identified transition areas using nearest neighbor interpolation ('d)' in the figure below)"),Object(s.b)("p",null,Object(s.b)("img",{src:a(191).default})),Object(s.b)("p",null,"A final label image is then stored in disk:"),Object(s.b)("p",null,Object(s.b)("img",{src:a(192).default})))}c.isMDXComponent=!0}}]);